{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.03: Filtering, Sorting, Combining, and Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following up on the last exercise, we are asked to deliver some more complex operations.   \n",
    "We will, therefore, continue to work with the same dataset, our `normal_distribution.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary dependencies\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "C:/Users/user/Desktop/ITD112/Datasets/normal_distribution_splittable.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loading the Dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/user/Desktop/ITD112/Datasets/normal_distribution_splittable.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m dataset\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:1977\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   1975\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1977\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1978\u001b[0m     fid_ctx \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mclosing(fid)\n\u001b[0;32m   1979\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: C:/Users/user/Desktop/ITD112/Datasets/normal_distribution_splittable.csv not found."
     ]
    }
   ],
   "source": [
    "# loading the Dataset\n",
    "dataset = np.genfromtxt('C:/Users/user/Desktop/ITD112/Datasets/normal_distribution_splittable.csv', delimiter=',')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get better insights into our dataset, we want to only look at the value that fulfills certain conditions.   \n",
    "Our client reaches out to us and asks us to provide lists of values that fulfills these conditions, filter down our dataset to contain only:\n",
    "- all values greater than 105 (>105)\n",
    "- all values that are between 90 and 95 (>90 and <95)\n",
    "- the indices of all values that have a delta of less than 1 to 100 (x-100 < 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values that are greater than 105\n",
    "vals_greater_five = dataset[dataset > 105]\n",
    "vals_greater_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values that are between 90 and 95\n",
    "vals_between_90_95 = np.extract((dataset > 90) & (dataset < 95), dataset)\n",
    "vals_between_90_95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**    \n",
    "Conditional filtering can be done either using the brackets syntax or NumPys `extract` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of values that have a delta of less than 1 to 100\n",
    "rows, cols = np.where(abs(dataset - 100) < 1)\n",
    "\n",
    "one_away_indices = [[rows[index], cols[index]] for (index, _) in np.ndenumerate(rows)]\n",
    "one_away_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also want to experiment with some more plotting techniques so they ask you to also deliver these datasets. Sort our dataset with:\n",
    "- values sorted in ascending order for each row\n",
    "- values sorted in ascending order for each column\n",
    "- the matrix of indices indicating the position in a sorted list of each value   \n",
    "```\n",
    "[3, 1, 2, 5, 4]  =>  [1, 2, 0, 4, 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values sorted for each row\n",
    "row_sorted = np.sort(dataset)\n",
    "row_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**   \n",
    "By default, sorting will always be done along the last axis. In our case this is 1, leading to each row being sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values sorted for each column\n",
    "col_sorted = np.sort(dataset, axis=0)\n",
    "col_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted indices of positions for first row\n",
    "index_sorted = np.argsort(dataset[0])\n",
    "dataset[0][index_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing their visualization and doing ask you to deliver a way they can incrementally add the split parts of the dataset to make sure it works with every subset, too.   \n",
    "Create a combined dataset by:\n",
    "- adding the second half of the first column\n",
    "- adding the second column\n",
    "- adding the third and last separate column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up dataset from exercise02\n",
    "thirds = np.hsplit(dataset, (3))\n",
    "halfed_first = np.vsplit(thirds[0], (2))\n",
    "\n",
    "# this is the part we've sent the client in exercise02\n",
    "halfed_first[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the second half of the first column to the data\n",
    "first_col = np.vstack([halfed_first[0], halfed_first[1]])\n",
    "first_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the second column to our combined dataset\n",
    "first_second_col = np.hstack([first_col, thirds[1]])\n",
    "first_second_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the third column to our combined dataset\n",
    "full_data = np.hstack([first_second_col, thirds[2]])\n",
    "full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**    \n",
    "The same results can be achieved with `np.concatenate` and `np.stack`.    \n",
    "For both methods, you need to provide the axis onto which it should be appended.   \n",
    "Depending on your preferences you might want to use those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For their internal AI algorithms, they need the dataset in a reshaped manner that reduces the number of columns.   \n",
    "They asked us to deliver the whole dataset in the following shapes. Create new datasets that are:\n",
    "- reshaped in a one-dimensional list with all values\n",
    "- reshaped in a matrix with only 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# reshaping to a list of values\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m single_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(\u001b[43mdataset\u001b[49m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      3\u001b[0m single_list \n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# reshaping to a list of values\n",
    "single_list = np.reshape(dataset, (1, -1))\n",
    "single_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# reshaping to a matrix with two columns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m two_col_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      3\u001b[0m two_col_dataset \n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# reshaping to a matrix with two columns\n",
    "two_col_dataset = dataset.reshape(-1, 2)\n",
    "two_col_dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**   \n",
    "-1 in the dimension definition means that it figures out the other dimension on its own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPH --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Filtering\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m vals_greater_five \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m[dataset \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m105\u001b[39m]\n\u001b[0;32m      3\u001b[0m vals_between_90_95 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mextract((dataset \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m90\u001b[39m) \u001b[38;5;241m&\u001b[39m (dataset \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m95\u001b[39m), dataset)\n\u001b[0;32m      4\u001b[0m rows, cols \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mabs\u001b[39m(dataset \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m100\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Filtering\n",
    "vals_greater_five = dataset[dataset > 105]\n",
    "vals_between_90_95 = np.extract((dataset > 90) & (dataset < 95), dataset)\n",
    "rows, cols = np.where(abs(dataset - 100) < 1)\n",
    "\n",
    "# Create histograms for filtered data\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.hist(vals_greater_five, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Values > 105')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(vals_between_90_95, bins=20, color='salmon', edgecolor='black')\n",
    "plt.title('Values between 90 and 95')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(dataset[rows, cols], bins=20, color='lightgreen', edgecolor='black')\n",
    "plt.title('Delta < 1 to 100')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sorting\n",
    "row_sorted = np.sort(dataset)\n",
    "col_sorted = np.sort(dataset, axis=0)\n",
    "index_sorted = np.argsort(dataset[0])\n",
    "\n",
    "# Create line plots for sorted data\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(row_sorted[0], marker='o', color='skyblue')\n",
    "plt.title('Values sorted in ascending order (Row 1)')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(col_sorted[:, 1], marker='o', color='salmon')\n",
    "plt.title('Values sorted in ascending order (Column 2)')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(index_sorted, marker='o', color='lightgreen')\n",
    "plt.title('Sorted Indices for Row 1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Combining\n",
    "combined_data = np.vstack([halfed_first[0], halfed_first[1]])\n",
    "\n",
    "# Create a heatmap for combined data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(combined_data, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Combined Data Heatmap')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
